# LDA Phase 4: Final Team Reflections

## Phase 4: Final Team Reflections (Weeks 17–20, Module 5 Sprints 1 & 2)

The goal of Phase 4 is to provide a dedicated space for teams to reflect on their entire LDA journey. The emphasis is not on defending a product, but on learning from the process, from each other, and from the feedback loop. This phase highlights the synthesis of learning from the discovery sprint to case study submission, analysis of feedback received during peer evaluation, insights derived from blind rankings (including surprises and alignment), reflection on team growth and challenges, and the translation of lessons into participants' real roles and agencies. Team presentation work begins in Module 5, Sprint 1, culminating in the final team presentation and LDA Retrospective deliverable in Module 5, Sprint 2.

### Facilitator Guide:

Facilitators host 15–20 minute team presentations, guiding teams to focus on reflection and learning rather than pitching a solution. Teams are encouraged to discuss honestly the challenges, growth, and surprises encountered throughout the LDA. This phase represents the crucial point where the experiential learning from the preceding three phases is consolidated and internalized. It is where the active engagement in "doing" (Phases 1 & 2) and "evaluating" (Phase 3) transforms into actionable understanding and application. By centering on reflection rather than defense, the program ensures participants extract maximum value from their successes and failures, fostering a growth mindset crucial for continuous professional development. This phase also implicitly validates the efficacy of the LDA's design in achieving its learning objectives, particularly vendor empathy and practical procurement skills.

Presentations must cover specific required elements:

- LDA Highlights: A brief recap of their case study submission (1–2 minutes) and the aspect of their work they are most proud of.
- Feedback Received: Discussion of what affirmed or surprised them from peer reviewers, and whether the feedback was clear, actionable, or eye-opening. Teams should also consider what they would improve if they revised their case study.
- Blind Rankings: Reflection on how their team’s ranking compared to expectations and whether the rank or rubric feedback highlighted anything unexpected about how their work was perceived.
- Evaluator Experience: Insights gained while reviewing other teams’ submissions, what made a case study stronger or harder to evaluate, and what they would incorporate into future solicitations or evaluations.
- Takeaways & Application: Identification of practices from the LDA that they will integrate into their real-world work, and how their view of digital procurement, collaboration, or discovery has changed.

Facilitators must ensure all team members speak during the presentation to promote collective reflection and shared learning. The use of visuals is encouraged to aid communication. Facilitators should leverage this phase to foster deep discussions about the challenges of evaluation, the importance of explicit evidence, and the impact of clarity on a "pass/fail" decision. This experience directly feeds into Phase 4's reflection on "Evaluator Experience" and "Takeaways & Application," solidifying the practical lessons learned from being on both sides of the procurement table. Facilitators should view Phase 4 not merely as a presentation, but as a critical assessment point for the program itself. The quality and depth of participant reflections can provide invaluable feedback for continuous improvement of the DITAP curriculum. It also serves as the final, reinforcing loop for the program's core message: bridging the gap between government and industry in digital services procurement. The "Evaluator Experience" and "Takeaways & Application" sections are particularly important for measuring the transfer of learning to real-world roles.

Team Deliverables: The primary deliverable for this phase is the Final Team Presentation and an LDA Retrospective.

### Assessment and Evaluation Framework

The overall assessment of the Live Digital Assignment is broken down into four weighted components:

- Case Study Package: 35%
- Peer Evaluation: 25%
- Presentation: 25%
- Reflections/Assessment: 15%

Facilitators are responsible for applying this assessment breakdown consistently across all teams, ensuring fairness and objectivity in grading. It is important to reiterate that the "Pass/Fail" evaluation in Phase 2 for the Case Study Package serves as a critical gate; successful completion of this phase is a prerequisite for further assessment and progression through the LDA.

### DITAP LDA Impact Summary

| Area of Growth | How LDA Delivers It |
| --- | --- |
| Empathy | Simulates vendor constraints & storytelling |
| Stakeholder Engagement | Research & interviews with real companies |
| Communication | Proposal writing, presentations, peer review |
| Agile & HCD Literacy | Aligns with Agile & user-centered methods |
| Technical Evaluation | Rubric-based peer evaluations |
| Readiness | Mirrors real government procurement cycles |


### Post-Award Management Simulation:

**Additional Presentation Component:**

Teams must address how they would manage the vendor relationship post-award:

* Sprint Planning: How government and vendor would collaborate on sprint planning
* Progress Monitoring: Key metrics and checkpoints for ongoing evaluation
* Change Management: Process for handling scope changes and user feedback
* Continuous Improvement: How lessons learned would be applied to future sprints

Reflection Questions:

* How would you balance vendor autonomy with government oversight?
* What red flags would indicate the need for corrective action?
* How would you ensure continuous value delivery to users?

### Conclusion

The Vendor Evaluation Live Digital Assignment for the DITAP training course represents a significant evolution in experiential learning for government acquisition professionals. By integrating independent acquisition discovery with a rigorous, fixed Case Study requirement aligned with the Case Study Evaluation framework, the program provides an unparalleled simulation of the digital services procurement lifecycle. This comprehensive approach not only builds essential technical skills in Agile procurement and evaluation but also cultivates a profound sense of empathy for vendors, a critical attribute for fostering more collaborative and successful government-industry partnerships. The structured, phase-based progression, coupled with the emphasis on self-directed learning and peer feedback, ensures that DITAP graduates are not merely knowledgeable but are truly ready to lead the transformation of digital service acquisition within their respective agencies.
