# LDA Phase 3: Peer Evaluation & Feedback 

## LDA Phase 3: Peer Evaluation & Feedback (Weeks 13–16, Module 4 Sprints 1 & 4)  

The objective of Phase 3 is for participants to simulate the technical evaluation process, assessing vendor submissions using a structured rubric. This reinforces their understanding of evaluation criteria, promotes fairness in scoring, and highlights the value of artifact-based reviews. Key learning outcomes for this phase include developing evaluation skills, fostering empathy for vendors, and practicing constructive critique. Teams commence reviewing anonymized packages in Module 4, Sprint 1, with deliverables including a completed rubric, evaluation memo, final ranked score, and feedback provided in Module 4, Sprint 4\.  

### Facilitator Guide
Facilitators manage the blind evaluation process, emphasizing its importance in eliminating bias and focusing solely on the submission's merit. Each team is assigned at least 3 anonymized case study packages for review. Depending on the size of the class, it could be that they receive all other team’s packages to review.   

This simulation of a government evaluation team utilizes real-world procurement techniques such as artifact-first down selection and blind review. The blind nature of the review is particularly impactful; it compels evaluators to focus purely on content and compliance, removing any personal bias or prior knowledge of the submitting team. This direct experience of evaluating others' work, especially after having just navigated the complexities of creating their own Case Study Evaluation-compliant submission, creates a powerful bridge of empathy. Participants gain firsthand understanding of the reviewer's perspective, appreciating what makes a submission clear, compliant, and easy to assess, and conversely, what leads to confusion or disqualification. This directly enhances their capacity for empathy towards vendors and improves their own future proposal writing.  

Facilitators provide and thoroughly review the standardized evaluation rubric, guiding teams on its consistent and objective application, with a focus on the Case Study Evaluation criteria. When providing feedback, teams are encouraged to use coaching-style language, focusing on actionable insights rather than mere criticism. They should explain precisely why a submission passed or failed based on specific Case Study Evaluation criteria. Facilitators also guide teams through the process of blind ranking submissions, encouraging discussion on what made a case study stronger or harder to evaluate. This reinforces the learning from both the vendor and evaluator perspectives.  

**Team Deliverables**: Each team is responsible for submitting one completed rubric and feedback memo per case reviewed, actively participating in feedback exchange, and contributing to a final ranked score.  

### Peer Evaluation Rubric Key Elements

| Criteria Category | Description |
| :---- | :---- |
| Description of Acquisition | Short introduction of the procurement, why it was chosen, and what the product being developed is.  |
| Artifact 1: Design File | Assessment of production-readiness and clear evidence of its use in usability research. |
| Artifact 2: Usability Research Report | Verification of evidence of one-on-one, task-based sessions (min. 4 participants) and clarity of insights/findings. |
| Artifact 3: Screenshot | Confirmation of a live product/feature screenshot showing a change directly based on research insights. |
| Artifact 4: Case Study Narrative (2-page) | Evaluation of the narrative's ability to provide project context, clearly explain the design process, and logically connect all artifacts. |
| Compliance/Clarity | Adherence to page limits, professional presentation, logical flow, and ease of understanding for the reviewer. |
| Pass/Fail Determination | Clear indication of whether the submission meets all minimum requirements as per Case Study Evaluation. |
| Feedback Quality | Assessment of feedback for constructiveness, actionability, and specificity to the evaluation criteria. |

### SCORING RUBRIC
(Can be updated per program)

| Component | Requirement Description | Pass Criteria | Fail Criteria |
| :---- | :---- | :---- | :---- |
| Artifact 1: Design File | A production-ready design file or prototype created for usability research with users. Format-agnostic (e.g., code, Figma, Sketch).   | Design file/prototype is production-ready AND clearly demonstrates its use for usability research with users.   | Design file/prototype is not production-ready, OR its use for usability research is not evident, OR it is missing.   |
| Artifact 2: Usability Research Report | A usability research report including evidence of one-on-one facilitated, task-based usability research sessions (remote or in-person) with a minimum of 4 participants. Must include research insights or findings from these sessions related to the product or feature shown in the Design File.   | Report includes evidence of one-on-one, task-based usability research sessions with at least 4 participants AND clearly presents research insights/findings directly related to the product/feature in the Design File.   | Report lacks evidence of required usability research (min. 4 participants), OR research is not one-on-one/task-based, OR insights/findings are missing/unclear, OR insights/findings do not relate to the Design File.   |
| Artifact 3: Screenshot | A screenshot of the live product or feature in a production environment, available for user use. Must show the implementation of a change based on the Usability Research Report insights or findings.   | Screenshot is of a live product/feature in a production environment AND clearly demonstrates an implemented change directly based on insights/findings from the Usability Research Report.   | Screenshot is not of a live product/feature, OR it is not in a production environment, OR it does not show an implemented change, OR the change is not clearly linked to the Usability Research Report insights/findings, OR it is missing.   |
| Artifact 4: Case Study Narrative (2-page) | A 2-page narrative describing the project context for Artifacts 1, 2, and 3\. Must include a clear explanation of the design process demonstrating the Design File was used for Usability Research, and that resulting insights were the basis for modification or addition of a design element released to production.   | Narrative is 2 pages AND clearly describes the project context for all three artifacts AND explicitly explains the design process, demonstrating the Design File's use in usability research, and how insights led to the implemented change in production.   | Narrative exceeds 2 pages, OR fails to describe project context for all artifacts, OR does not clearly explain the design process, OR fails to demonstrate the link between Design File, Usability Research, insights, and implemented change, OR it is missing.   |
| Overall Submission | The complete Case Study Package must conform to all specified conditions for each artifact. No assumptions of conformance will be made.   | All four artifacts (Design File, Usability Research Report, Screenshot, and 2-page Narrative Case Study) are submitted and each individually meets all "Pass Criteria" as outlined above.   | Any single artifact fails to meet its "Pass Criteria," OR any required component is missing, OR the submission requires assumptions to confirm conformance.   |
