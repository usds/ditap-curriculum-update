# LDA Phase 2: Case Study Development 

## Phase 2: Case Study Development (Weeks 7–12, Module 3 Sprints 1 & 4\)

The objective of Phase 2 is to provide participants with hands-on experience responding to an Agile-focused Request for Quotation (RFQ) using real-world digital service evaluation criteria. Teams simulate vendors by creating a comprehensive case study package that highlights design thinking, usability testing, and implementation. Key learning outcomes for this phase include Agile procurement and proposal writing. This phase builds directly on the Discovery Sprint, with teams developing a case study in response to their RFQ, starting in Module 3, Sprint 1, and concluding with the submission of the Case Study with artifacts in Module 3, Sprint 4\.

### Facilitator guide

Facilitators must guide teams on how to effectively map their independently discovered project from Phase 1 to the specific, fixed Case Study Evaluation-aligned Case Study requirements. The suitability of the initial project selection becomes paramount at this stage. Facilitators must ensure that teams understand that while their Phase 1 research provides the context and problem, Phase 2 demands a specific type of vendor response that meticulously aligns with the Case Study Evaluation framework.

***Detailed guidance is essential for creating each of the four required artifacts***:

* **Artifact 1**: Design File or Prototype: Teams must develop a production-ready design file or prototype that was specifically used in usability research with users. This file is format-agnostic and can be created using various tools such as code, Figma, or Sketch.  
* **Artifact 2**: Usability Research Summary \- This report must include evidence of one-on-one, facilitated, task-based usability research sessions conducted with a minimum of four participants. The report must present research insights or findings directly related to the product or feature shown in the Design File.  
* **Artifact 3**: Screenshot of Implemented Change: A screenshot of the live product or feature in a production environment is required. This screenshot must visually demonstrate the implementation of a change that is directly based on the insights or findings from the Usability Research Report. This causal link between research and implementation is a critical element for evaluation.  
* **Artifact 4**: 2-Page Narrative Case Study \- This concise document should describe the project context for the other three artifacts. It must include a clear explanation of the design process, demonstrating that the Design File was used for Usability Research, and that the resulting insights formed the basis for modifying or adding a design element subsequently released to production. Teams are prompted to ensure their narrative walks the reviewer through this story without requiring assumptions.

  **Enhanced Artifact 4 requirements:** <br>
   The 2-page case study must also include:
  * **Modular Approach:** How the solution could be broken into discrete, independently valuable modules
  * **Performance Metrics:** Specific, measurable outcomes (not just outputs) that demonstrate user value
  * **Risk Mitigation:** Key technical and programmatic risks and proposed mitigation strategies
  * **Continuous Delivery:** How the solution would support ongoing iteration and improvement
* **Artifact 5:** Agile Contracting Approach
  * Describe how the work would be structured using Agile contracting methods.
  * Define sprint length, delivery cadence, and key performance indicators (KPIs).
  * Outline how user feedback would be integrated into ongoing development.
  * Specify acceptance criteria for deliverables.
* **Artifact 6:** Security & Compliance Summary (1 page) - Document key security controls, compliance considerations, and how security testing was integrated into the user research and development process.
  * _Facilitator Guidance:_
Emphasize that security is not an afterthought but integral to user experience. Guide teams to consider how security requirements affect user research design and implementation decisions.
  
A critical aspect for facilitators to emphasize is the "Pass/Fail" evaluation criteria for Round 1, as explicitly outlined in the Case Study Evaluation requirements. This evaluation is not comparative; it simply assesses whether the submission meets the bar. Any missing component, lack of detail, or unclear connection will result in a "Fail". This strict gate serves as a powerful pedagogical tool, simulating the harsh realities of initial government procurement stages where meticulous proposal writing and strict adherence to solicitation requirements are paramount. The "no assumptions" clause in the Case Study Evaluation framework is particularly impactful, mirroring actual government evaluation practices where ambiguity or omission leads to disqualification. This high-stakes environment directly contributes to cultivating empathy for vendors, as participants experience the pressure of meeting stringent, non-negotiable criteria. Facilitators must consistently remind teams of this "pass/fail" reality, emphasizing precision, clarity, and the explicit connection between artifacts.

Teams are also informed that their submissions will be evaluated blindly by other teams in Phase 3\. This encourages them to create clear, well-organized, and easily understandable packages, making it easy for a reviewer to follow their logic, trace insights to design, and connect implementation to research.

### Team deliverables 
Teams must submit a complete Case Study Package comprising all four artifacts. This deliverable is a key milestone in Module 3, Sprint 4\.

### Case study evaluation
Case Study Artifact Requirements & Evaluation Criteria

**Technical Factor 1 - Case Study #1**

| Description of Acquisition | Short introduction of the procurement, why it was chose, and what the product being developed is |
| :---- | :---- |
| Artifact 1: Design File | A production-ready design file or prototype created for usability research with users. Format-agnostic (code, Figma, Sketch, etc.). |
| Artifact 2: Usability Research Report | Includes evidence of one-on-one facilitated, task-based usability research sessions (remote or in-person) with a minimum of 4 participants. Must include research insights or findings from these sessions related to the product or feature shown in the Design File. |
| Artifact 3: Screenshot | A screenshot of the live product or feature in a production environment, available for user use. Must show the implementation of a change based on the Usability Research Report insights or findings. |
| Artifact 4: Case Study Narrative (2-page) | Describes the project context for Artifacts 1, 2, and 3\. Must include a clear explanation of the design process demonstrating the Design File was used for Usability Research, and that resulting insights were the basis for modification or addition of a design element released to production. |

### Evaluation criteria

**Pass**: If the Offeror’s Case Study and Artifacts conform with each of the conditions above. No credit for responses exceeding minimum requirements. No assumptions of conformance will be made.Documentation supports evaluation decisions.</br>

**Fail**: If any responses are not confirmed to conform with each of the conditions above. Documentation supports evaluation decisions.



### Security and compliance integration:

**Mandatory security considerations:**

All case study submissions must address:
* **Security by Design:** How security requirements were integrated from the start
* **Compliance Framework:** Relevant federal compliance requirements (FedRAMP, FISMA, etc.)
* **DevSecOps Integration:** How security testing and monitoring would be continuous
* **Data Protection:** Approach to protecting sensitive user data and PII

### Performance-based contracting framework:

**Outcome-focused metrics:** 

Case studies must define success in terms of user outcomes, not technical outputs:
* User Impact Metrics: How success would be measured from the user's perspective
* Business Value Metrics: Quantifiable benefits to the agency mission
* Performance Standards: Specific, measurable criteria for acceptable performance
* Incentive Structure: How vendor performance would be rewarded or penalized

**Enhanced Evaluation Criteria:**

Submissions must demonstrate clear linkage between user research insights and measurable performance outcomes.
