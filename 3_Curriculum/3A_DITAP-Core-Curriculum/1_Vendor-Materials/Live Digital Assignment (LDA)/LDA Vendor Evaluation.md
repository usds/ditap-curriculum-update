# DITAP Vendor Evaluation LDA

As part of the DITAP program, participants complete a project that simulates the end-to-end lifecycle of a modern digital service acquisition. Known as the Live Digital Assignment (LDA), this experience reinforces core DITAP objectives by immersing participants in real-world procurement practices.

The LDA fosters empathy by engaging participants in vendor constraints and the storytelling skills required to craft compelling proposals. It strengthens stakeholder engagement through interviews with real agencies or companies, and sharpens communication skills through proposal writing, peer review, and formal presentations. Participants apply Agile and Human-Centered Design (HCD) principles throughout, deepening their ability to scope problems, engage vendors, and assess technical solutions.

A key enhancement in this LDA model is the shift toward independent project discovery. Rather than working from a pre-written case, participant teams identify their own real-world digital service procurement challenge that meets a shared set of criteria. This mirrors how acquisition professionals begin by researching the landscape and identifying mission-driven needs before issuing solicitations. While each team’s project is self-defined, all responses follow a fixed format aligned to a Technical Factor from the VA’s SPRUCE solicitation.

Facilitators and program managers play a vital role in helping participants apply lessons from the LDA to their day-to-day work, thereby bridging the gap between learning and impact across government agencies.

**The LDA is structured into four phases**:
- Discovery Sprint  
- Case Study Development  
- Evaluation & Feedback  
- Final Presentations

**Learning Objectives Reinforced**:
* Scoping problems using Human-Centered Design and Discovery Sprint methods  
* Applying modern, Agile-friendly acquisition practices  
* Conducting market research and engaging vendors empathetically  
* Evaluating digital service proposals based on artifact-first approaches  
* Delivering constructive feedback and collaborating cross-functionally

## LDA Timeline & Phases

| Phase | Weeks | Focus | Deliverables | Key Learning Outcome |
| :---- | :---- | :---- | :---- | :---- |
| Phase 1 | Weeks 2–6 | Discovery Sprint | Discovery Summary | Human-centered discovery, framing |
| Phase 2 | Weeks 7–12 | Case Study Development | Case Study Package | Agile procurement, proposal writing |
| Phase 3 | Weeks 13–16 | Review of other teams’ case studies | Rubric \+ Feedback \+ Blind Ranking | Evaluation skills, empathy for vendors, constructive critique |
| Phase 4 | Weeks 17–20 | Present what you learned, not what you built | Team Presentation & Reflections | Team reflection, learning insights, and future application |

## Phase 1: Discovery Sprint Simulation

Objective: Introduce participants to human-centered problem scoping using the Discovery Sprint model. This phase simulates how agencies and vendors define problems before writing solicitations, emphasizing stakeholder engagement, constraints, and opportunity identification. Teams must select a digital service project that enables the creation or thoughtful simulation of key artifacts used later in the LDA. Facilitators should help teams validate their selections early to ensure the project supports the full LDA lifecycle, reinforcing the continuity and real-world relevance of user-centered acquisition strategy.

**Instructions to Facilitators**:
* Form teams of 4–5.  
* Assign or allow teams to select a recent (within 2–3 years) digital service project to analyze.    
* Require each team to conduct 2 simulated stakeholder interviews (via AI tools, facilitator-provided scenarios, or role-play).  
* Provide excerpts or summaries of the [USDS Discovery Sprint Guide](https://sprint.usds.gov/).

**Team Deliverables**:
* 3-page Discovery Summary that includes:  
  * Background & problem framing  
  * Stakeholders identified  
  * Research and insights  
  * Pain points and opportunities

**Facilitator Prompt**: “You’re the Vendor—Make It to the Next Round”

You’re now stepping into the shoes of a digital services company responding to a real government RFQ. This is Phase 2 of your LDA, and your goal is to submit a complete, clear, and compliant case study package that demonstrates your ability to deliver user-centered, iterative digital services.

The solicitation you’re responding to uses a pass/fail evaluation for Round 1\. This means the government is not comparing you to other companies yet—they’re simply asking:  
 Did you meet the bar or not?

Your task is to submit a comprehensive, understandable, and compliant package of artifacts and a case study that meets all the listed requirements. If you miss a piece, lack detail, or fail to tie things together clearly, you will fail. If you do everything well, you pass and move on.

So ask yourself as you build your package:
* Does our design file connect to the usability research we conducted?  
* Do our insights logically lead to the implemented feature we’re showing?  
* Does our two-page narrative guide the reviewer through this story without making assumptions?  
* Are we treating this like a real submission—professional, focused, and tight?

This is a simulation, yes—but treat it seriously. You’re building your response like your company’s future depends on it. Because in this scenario? It does.

**You will submit four artifacts**:
1. A design file or prototype used in research  
2. A usability research summary that shows your insights  
3. A visual or screenshot showing an implemented change  
4. A 2-page narrative case study connecting the dots between them

Think of this like Round 1 of a real procurement: if anything is missing, vague, or disconnected, you don’t cut.</br>
But here’s the twist…</br>
- In Phase 3, you’ll step into the role of a government evaluator and review other teams’ submissions—completely blind, with no team names.
- You’ll apply a rubric, provide constructive feedback, and determine whether those teams pass or fail. So as you build your own response now in Phase 2, remember:

**Other teams will be evaluating your work**.
* Are you making it easy for a reviewer to follow your logic?  
* Are your insights tied to your design?  
* Is your implementation traceable to your research?  
* Is it professional, well-organized, and within scope?

Bottom Line:

Your team’s job right now is to submit a case study that passes the bar.

* Phase 1 was your discovery and research.  
* Phase 2 is your formal response.  
* Phase 3 is where you’ll find out what it’s like to sit on the other side of the table. 

### Phase 2: Case Study Development

Objective: Give participants experience responding to an Agile-focused RFQ using real-world digital service evaluation criteria. Teams simulate vendors by creating a case study package that highlights design thinking, usability testing, and implementation.

**Instructions to Facilitators**:
* Introduce the SPRUCE-aligned RFQ Case Study:  
  * One project, four artifacts:  
    * **Artifact 1**: Design File or clickable prototype  
    * **Artifact 2**: Usability Research Summary  
    * **Artifact 3**: Screenshot or evidence of implementation  
    * **Artifact 4**: 2-page Case Study tying everything together

**Team Deliverables**:
* Submit all four artifacts as a complete Case Study Package

### Phase 3: Peer Evaluation & Feedback

Objective: Participants simulate the technical evaluation process, assessing vendor submissions using a structured rubric. This reinforces understanding of evaluation criteria, fairness in scoring, and the value of artifact-based reviews. Emphasize that this is a simulation of a government evaluation team using real-world procurement techniques such as artifact-first down selection and blind review.

**Instructions to Facilitators**:
* Assign each team to evaluate 2-3 anonymized case study packages from other teams  
* Use a standardized evaluation rubric to assess each one  
* Provide written feedback to each team using coaching-style language  
* Rank the submissions blindly (no team names)

**Team Deliverables**:
* 1 completed rubric \+ feedback memo per case reviewed  
* Participate in feedback exchange

### Phase 4: Final Team Reflections

Objective: The goal of Phase 4 is to create space for teams to reflect on their full LDA journey, not to defend a product, but to learn from the process, each other, and the feedback loop. Teams will revisit their discovery and case study work, discuss how their submission was evaluated by peers, and explore their own role as evaluators.

**This phase emphasizes**:
* Synthesis of learning from the discovery sprint to case study submission  
* Analysis of feedback received during the peer evaluation phase  
* Insights from blind rankings, including surprises and alignment  
* Reflection on team growth, challenges, and empathy  
* Translation of lessons into participants' real roles and agencies

**Instructions to Facilitators**:
* Host 15–20 minute team presentations
    - In this final phase, each team will give a 10–15 minute presentation, not to pitch a solution, but to reflect on what you’ve learned across the whole LDA experience.
    - Use this as a chance to talk honestly about challenges, growth, and surprises. 

**Each team presentation should include**:
* LDA Highlights  
  * Quick recap of your case study submission (1–2 minutes)  
  * What aspect of your work you’re most proud of  
* Feedback Received  
  * What did you hear from peer reviewers that affirmed or surprised you?  
  * Was the feedback clear, actionable, or eye-opening?  
  * What would you improve if you revised your case study?  
* Blind Rankings  
  * How did your team’s ranking compare to your expectations?  
  * Did the rank or rubric feedback highlight anything unexpected about how your work was perceived?  
* Evaluator Experience  
  * What did you notice while reviewing others’ submissions?  
  * What made a case study stronger or harder to evaluate?  
  * What would you take into future solicitations or evaluations?  
* Takeaways & Application  
  * What practices from this LDA will you bring into your real-world work?  
  * How do you view digital procurement, collaboration, or discovery differently now?

**Presentation Requirements**:
* Visuals encouraged  
* All team members speak

**Assessment Breakdown**:
* Case Study Package: 35%  
* Peer Evaluation: 25%  
* Presentation: 25%  
* Reflections/Assessment: 15%

**Support Materials to Distribute**:
* LDA Orientation Slides  
* Discovery Sprint Guide (USDS or summary)  
* Interview Templates \+ Stakeholder Tips  
* Case Study Template (aligned with SPRUCE)  
* Artifact Writing Guide  
* Blind Evaluation Rubric Template

## LDA Impact Summary

| Area of Growth | How LDA Delivers It |
| :---- | :---- |
| Empathy | Simulates vendor constraints & storytelling |
| Stakeholder Engagement | Research & interviews with real companies |
| Communication | Proposal writing, presentations, peer review |
| Agile & HCD Literacy | Aligns with Agile & user-centered methods |
| Technical Evaluation | Rubric-based peer evaluations |
| Readiness | Mirrors real government procurement cycles |

## VA Spruce Case Study \#1: Actual Text  
Technical Factor 1 - Case Study \#1. Offerors must submit the items listed below. The Government will use these items to evaluate the Offeror’s conformance with the minimum requirements for Technical Factor 1\. Items must be from the same project, which shall be described in the Case Study.
- **Artifact 1** - Design File: A production-ready design file or prototype that was created for usability research with users. This file is format-agnostic and can be created in code, Figma, Sketch, or other similar tools.
- **Artifact 2** – Usability Research Report: A usability research report that includes evidence of one-on-one facilitated, task-based, usability research sessions (either remote or in-person) with a minimum of 4 participants. The report must include research insights or findings from these sessions related to the product or feature shown in the Design File.
- **Artifact 3** – Screenshot: A screenshot of the live product or feature in a production environment where it is available for use by users. The screenshot must display the implementation of a change based on the insights or findings from the Usability Research Report.
- **Case Study**: A 2-page case study describing the project context for Artifacts 1, 2 and 3. The Case Study must include a clear explanation of the design process that demonstrates the Design File was used for the Usability Research and that the resulting insights or findings were the basis for the modification or addition of a design element that was released to production for users (Item c).

Offerors are advised that they won't be given credit for responses exceeding the minimum requirements in items above. Technical Factor 1 will be evaluated as either Pass or Fail. Offerors will receive a “Pass” if the Offeror’s Case Study and Artifacts are confirmed to conform to each of the conditions above. It is incumbent on Offerors to ensure sufficient detail is provided in their Technical Factor 1 response to allow the Government to evaluate conformance with these requirements. The VA will make no assumptions of conformance. Offerors will receive a “Fail” if any responses are not confirmed to conform to each of the conditions above. Offerors who receive a “Fail” will not have their remaining proposal Volumes evaluated and will be ineligible for award. 

After Technical Factor 1 Evaluations are completed, those Offerors who receive a “Pass” will be notified accordingly, and the Government will begin evaluation of Technical Factor 2\. Offerors who receive a “Fail” will be notified accordingly.

## Phase-by-Phase Implementation Guide: Detailed Facilitator Instructions

This section provides comprehensive, actionable guidance for facilitators across the four LDA phases, emphasizing the integration of the new independent acquisition discovery element and reinforcing the fixed Case Study requirements.

### Phase 1: Discovery Sprint Simulation (Weeks 2–6, Module 2 Sprints 1 & 4\)

The objective of Phase 1 is to introduce participants to human-centered problem scoping utilizing the Discovery Sprint model. This phase simulates how agencies and vendors define problems before writing solicitations, with a strong emphasis on stakeholder engagement, constraint analysis, and opportunity identification. Key learning outcomes include human-centered discovery and framing. This phase commences with the identification and approval of an appropriate product in Module 2, Sprint 1, culminating in a Sprint Report deliverable in Module 2, Sprint 4\.

### Facilitator Guide:

Facilitators begin by forming teams of 4 to 5 participants. A crucial aspect of this phase involves guiding teams on the specific characteristics of a recent (within 2–3 years) digital service project suitable for analysis. This guidance must explicitly connect to the types of projects that can generate the Vendor Case Study-aligned artifacts required for Phase 2\. Facilitators should emphasize the importance of identifying projects where user research was demonstrably conducted and where a clear, iterative change was implemented based on that research.

The facilitator's role here extends beyond simple approval; it involves actively assessing whether the chosen project can realistically yield the necessary Vendor Case Study artifacts. If a team selects a project lacking a clear user research component or a demonstrable iteration based on feedback, it risks setting the team up for failure in Phase 2\. Therefore, the facilitator acts as a compatibility gatekeeper, ensuring the selected project possesses the inherent characteristics to support the subsequent phases. This may involve probing questions such as: "Can a specific feature or design element that underwent user research be identified?" or "Is there evidence of an iterative change based on user feedback?" This proactive guidance, provided early in Phase 1, is fundamental to preventing teams from investing time in a project that cannot meet the Phase 2 requirements, thereby ensuring the cumulative learning experience remains viable.

Teams are encouraged to conduct outreach to real digital service companies or agency partners for stakeholder interviews but are required to conduct 2 simulated stakeholder interviews (via AI tools, facilitator-provided scenarios, or role-play). Facilitators should provide strategies and templates for these interactions, highlighting the value of gaining authentic insights into problem definition and stakeholder perspectives. Teams are required to conduct 2 simulated stakeholder interviews, with facilitators providing scenarios or role-playing exercises if real interviews are not universally feasible. Participants should utilize provided excerpts or summaries of the USDS Discovery Sprint Guide to effectively frame their problem scoping and research activities. Finally, facilitators must explicitly link the Phase 1 Discovery Summary to the upcoming Case Study Development, explaining that the research conducted in this phase forms the foundational context for the vendor response in Phase 2\.

### Team Deliverables: 
Each team is required to submit a 3-page Discovery Summary. This document must include a background and problem framing, identification of stakeholders, research and insights, and a discussion of pain points and opportunities. This deliverable is also referred to as a Sprint Report.

## Phase 1 Facilitator Checklist & Key Prompts

| Task | Key Prompts/Considerations |
| :---- | :---- |
| Form Teams | Ensure equitable distribution of skills. |
| Introduce Independent Discovery | Explain the rationale for self-selection and its connection to real-world scenarios. |
| Guide Project Selection (Criteria for Case Study Evaluation compatibility) | "Does the chosen project allow for demonstration of a design file used in usability research?" "Can an implemented change based on research insights be clearly identified or simulated?" "Are teams aware that this discovery will directly feed into a fixed, pass/fail Case Study requirement?" |
| Encourage Outreach | "Have teams identified key stakeholders for interviews, and are their insights relevant to a digital service problem?" Provide templates and best practices for professional engagement. |
| Facilitate Interviews | Guide teams on conducting effective interviews and provide simulated scenarios as needed. |
| Review Discovery Summary | "Is the Discovery Summary clearly articulating a problem that a digital service solution could address?" Assess clarity, completeness, and alignment with HCD principles. |

## Phase 2: Case Study Development (Weeks 7–12, Module 3 Sprints 1 & 4\)

The objective of Phase 2 is to provide participants with hands-on experience responding to an Agile-focused Request for Quotation (RFQ) using real-world digital service evaluation criteria. Teams simulate vendors by creating a comprehensive case study package that highlights design thinking, usability testing, and implementation. Key learning outcomes for this phase include Agile procurement and proposal writing. This phase builds directly on the Discovery Sprint, with teams developing a case study in response to their RFQ, starting in Module 3, Sprint 1, and concluding with the submission of the Case Study with artifacts in Module 3, Sprint 4\.

### Facilitator Guide:

Facilitators must guide teams on how to effectively map their independently discovered project from Phase 1 to the specific, fixed Case Study Evaluation-aligned Case Study requirements. The suitability of the initial project selection becomes paramount at this stage. Facilitators must ensure that teams understand that while their Phase 1 research provides the context and problem, Phase 2 demands a specific type of vendor response that meticulously aligns with the Case Study Evaluation framework.

Detailed guidance is essential for creating each of the four required artifacts:
* Artifact 1: Design File or Prototype: Teams must develop a production-ready design file or prototype that was specifically used in usability research with users. This file is format-agnostic and can be created using various tools such as code, Figma, or Sketch.  
* Artifact 2: Usability Research Summary \- This report must include evidence of one-on-one, facilitated, task-based usability research sessions conducted with a minimum of four participants. The report must present research insights or findings directly related to the product or feature shown in the Design File.  
* Artifact 3: Screenshot of Implemented Change: A screenshot of the live product or feature in a production environment is required. This screenshot must visually demonstrate the implementation of a change that is directly based on the insights or findings from the Usability Research Report. This causal link between research and implementation is a critical element for evaluation.  
* Artifact 4: 2-Page Narrative Case Study \- This concise document should describe the project context for the other three artifacts. It must include a clear explanation of the design process, demonstrating that the Design File was used for Usability Research, and that the resulting insights formed the basis for the modification or addition of a design element subsequently released to production. Teams are prompted to ensure their narrative walks the reviewer through this story without requiring assumptions.

A critical aspect for facilitators to emphasize is the "Pass/Fail" evaluation criteria for Round 1, as explicitly outlined in the Case Study Evaluation requirements. This evaluation is not comparative; it simply assesses whether the submission meets the bar. Any missing component, lack of detail, or unclear connection will result in a "Fail". This strict gate serves as a powerful pedagogical tool, simulating the harsh realities of initial government procurement stages where meticulous proposal writing and strict adherence to solicitation requirements are paramount. The "no assumptions" clause in the Case Study Evaluation framework is particularly impactful, mirroring actual government evaluation practices where ambiguity or omission leads to disqualification. This high-stakes environment directly contributes to cultivating empathy for vendors, as participants experience the pressure of meeting stringent, non-negotiable criteria. Facilitators must consistently remind teams of this "pass/fail" reality, emphasizing precision, clarity, and the explicit connection between artifacts.

Teams are also informed that their submissions will be evaluated blindly by other teams in Phase 3\. This encourages them to create clear, well-organized, and easily understandable packages, making it easy for a reviewer to follow their logic, trace insights to design, and connect implementation to research.

### Team Deliverables: 

Teams must submit a complete Case Study Package comprising all four artifacts. This deliverable is a key milestone in Module 3, Sprint 4\.

## Case Study Evaluation: Case Study Artifact Requirements & Evaluation Criteria

| Technical Factor 1 – Case Study \#1 (Pass/Fail Evaluation) |
| :---- |
| Description of Acquisition |
| Short introduction of the procurement, why it was chosen, and what the product being developed is.  |
| Artifact 1: Design File |
| A production-ready design file or prototype created for usability research with users. Format-agnostic (code, Figma, Sketch, etc.). |
| Artifact 2: Usability Research Report |
| Includes evidence of one-on-one facilitated, task-based usability research sessions (remote or in-person) with a minimum of 4 participants. Must include research insights or findings from these sessions related to the product or feature shown in the Design File. |
| Artifact 3: Screenshot |
| A screenshot of the live product or feature in a production environment, available for user use. Must show the implementation of a change based on the Usability Research Report insights or findings. |
| Artifact 4: Case Study Narrative (2-page) |
| Describes the project context for Artifacts 1, 2, and 3\. Must include a clear explanation of the design process demonstrating the Design File was used for Usability Research, and that resulting insights were the basis for modification or addition of a design element released to production. |
| Evaluation Criteria: |
| Pass: If the Offeror’s Case Study and Artifacts conform with each of the conditions above. No credit for responses exceeding minimum requirements. No assumptions of conformance will be made.Documentation supports evaluation decisions. |
| Fail: If any responses are not confirmed to conform with each of the conditions above. Documentation supports evaluation decisions. |

## Phase 3: Peer Evaluation & Feedback (Weeks 13–16, Module 4 Sprints 1 & 4\)

The objective of Phase 3 is for participants to simulate the technical evaluation process, assessing vendor submissions using a structured rubric. This reinforces their understanding of evaluation criteria, promotes fairness in scoring, and highlights the value of artifact-based reviews. Key learning outcomes for this phase include developing evaluation skills, fostering empathy for vendors, and practicing constructive critique. Teams commence reviewing anonymized packages in Module 4, Sprint 1, with deliverables including a completed rubric, evaluation memo, final ranked score, and feedback provided in Module 4, Sprint 4\.

### Facilitator Guide:

Facilitators manage the blind evaluation process, emphasizing its importance in eliminating bias and focusing solely on the submission's merit. Each team is assigned at least 3 anonymized case study packages for review. Depending on the size of the class, it could be that they receive all other team’s packages to review. 

This simulation of a government evaluation team utilizes real-world procurement techniques such as artifact-first down selection and blind review. The blind nature of the review is particularly impactful; it compels evaluators to focus purely on content and compliance, removing any personal bias or prior knowledge of the submitting team. This direct experience of evaluating others' work, especially after having just navigated the complexities of creating their own Case Study Evaluation-compliant submission, creates a powerful bridge of empathy. Participants gain firsthand understanding of the reviewer's perspective, appreciating what makes a submission clear, compliant, and easy to assess, and conversely, what leads to confusion or disqualification. This directly enhances their capacity for empathy towards vendors and improves their own future proposal writing.

Facilitators provide and thoroughly review the standardized evaluation rubric, guiding teams on its consistent and objective application, with a focus on the Case Study Evaluation criteria. When providing feedback, teams are encouraged to use coaching-style language, focusing on actionable insights rather than mere criticism. They should explain precisely why a submission passed or failed based on specific Case Study Evaluation criteria. Facilitators also guide teams through the process of blind ranking submissions, encouraging discussion on what made a case study stronger or harder to evaluate. This reinforces the learning from both the vendor and evaluator perspectives.

### Team Deliverables: 
Each team is responsible for submitting one completed rubric and feedback memo per case reviewed, actively participating in feedback exchange, and contributing to a final ranked score.

## Peer Evaluation Rubric Key Elements

| Criteria Category | Description |
| :---- | :---- |
| Description of Acquisition | Short introduction of the procurement, why it was chosen, and what the product being developed is.  |
| Artifact 1: Design File | Assessment of production-readiness and clear evidence of its use in usability research. |
| Artifact 2: Usability Research Report | Verification of evidence of one-on-one, task-based sessions (min. 4 participants) and clarity of insights/findings. |
| Artifact 3: Screenshot | Confirmation of a live product/feature screenshot showing a change directly based on research insights. |
| Artifact 4: Case Study Narrative (2-page) | Evaluation of the narrative's ability to provide project context, clearly explain the design process, and logically connect all artifacts. |
| Compliance/Clarity | Adherence to page limits, professional presentation, logical flow, and ease of understanding for the reviewer. |
| Pass/Fail Determination | Clear indication of whether the submission meets all minimum requirements as per Case Study Evaluation. |
| Feedback Quality | Assessment of feedback for constructiveness, actionability, and specificity to the evaluation criteria. |

## Scoring Rubric 
(Can be updated per program)

| Component | Requirement Description | Pass Criteria | Fail Criteria |
| :---- | :---- | :---- | :---- |
| Artifact 1: Design File | A production-ready design file or prototype created for usability research with users. Format-agnostic (e.g., code, Figma, Sketch).   | Design file/prototype is production-ready AND clearly demonstrates its use for usability research with users.   | Design file/prototype is not production-ready, OR its use for usability research is not evident, OR it is missing.   |
| Artifact 2: Usability Research Report | A usability research report including evidence of one-on-one facilitated, task-based usability research sessions (remote or in-person) with a minimum of 4 participants. Must include research insights or findings from these sessions related to the product or feature shown in the Design File.   | Report includes evidence of one-on-one, task-based usability research sessions with at least 4 participants AND clearly presents research insights/findings directly related to the product/feature in the Design File.   | Report lacks evidence of required usability research (min. 4 participants), OR research is not one-on-one/task-based, OR insights/findings are missing/unclear, OR insights/findings do not relate to the Design File.   |
| Artifact 3: Screenshot | A screenshot of the live product or feature in a production environment, available for user use. Must show the implementation of a change based on the Usability Research Report insights or findings.   | Screenshot is of a live product/feature in a production environment AND clearly demonstrates an implemented change directly based on insights/findings from the Usability Research Report.   | Screenshot is not of a live product/feature, OR it is not in a production environment, OR it does not show an implemented change, OR the change is not clearly linked to the Usability Research Report insights/findings, OR it is missing.   |
| Artifact 4: Case Study Narrative (2-page) | A 2-page narrative describing the project context for Artifacts 1, 2, and 3\. Must include a clear explanation of the design process demonstrating the Design File was used for Usability Research, and that resulting insights were the basis for modification or addition of a design element released to production.   | Narrative is 2 pages AND clearly describes the project context for all three artifacts AND explicitly explains the design process, demonstrating the Design File's use in usability research, and how insights led to the implemented change in production.   | Narrative exceeds 2 pages, OR fails to describe project context for all artifacts, OR does not clearly explain the design process, OR fails to demonstrate the link between Design File, Usability Research, insights, and implemented change, OR it is missing.   |
| Overall Submission | The complete Case Study Package must conform to all specified conditions for each artifact. No assumptions of conformance will be made.   | All four artifacts (Design File, Usability Research Report, Screenshot, and 2-page Narrative Case Study) are submitted and each individually meets all "Pass Criteria" as outlined above.   | Any single artifact fails to meet its "Pass Criteria," OR any required component is missing, OR the submission requires assumptions to confirm conformance.   |

## Phase 4: Final Team Reflections (Weeks 17–20, Module 5 Sprints 1 & 2\)

The goal of Phase 4 is to provide a dedicated space for teams to reflect on their entire LDA journey. The emphasis is not on defending a product, but on learning from the process, from each other, and from the feedback loop. This phase highlights the synthesis of learning from the discovery sprint to case study submission, analysis of feedback received during peer evaluation, insights derived from blind rankings (including surprises and alignment), reflection on team growth and challenges, and the translation of lessons into participants' real roles and agencies. Team presentation work begins in Module 5, Sprint 1, culminating in the final team presentation and LDA Retrospective deliverable in Module 5, Sprint 2\.

### Facilitator Guide:

Facilitators host 15–20 minute team presentations, guiding teams to focus on reflection and learning rather than pitching a solution. Teams are encouraged to discuss honestly the challenges, growth, and surprises encountered throughout the LDA. This phase represents the crucial point where the experiential learning from the preceding three phases is consolidated and internalized. It is where the active engagement in "doing" (Phases 1 & 2\) and "evaluating" (Phase 3\) transforms into actionable understanding and application. By centering on reflection rather than defense, the program ensures participants extract maximum value from their successes and failures, fostering a growth mindset crucial for continuous professional development. This phase also implicitly validates the efficacy of the LDA's design in achieving its learning objectives, particularly vendor empathy and practical procurement skills.

**Presentations must cover specific required elements**:
* LDA Highlights: A brief recap of their case study submission (1–2 minutes) and the aspect of their work they are most proud of.  
* Feedback Received: Discussion of what affirmed or surprised them from peer reviewers, and whether the feedback was clear, actionable, or eye-opening. Teams should also consider what they would improve if they revised their case study.  
* Blind Rankings: Reflection on how their team’s ranking compared to expectations and whether the rank or rubric feedback highlighted anything unexpected about how their work was perceived.  
* Evaluator Experience: Insights gained while reviewing other teams’ submissions, what made a case study stronger or harder to evaluate, and what they would incorporate into future solicitations or evaluations.  
* Takeaways & Application: Identification of practices from the LDA that they will integrate into their real-world work, and how their view of digital procurement, collaboration, or discovery has changed.

Facilitators must ensure all team members speak during the presentation to promote collective reflection and shared learning. The use of visuals is encouraged to aid communication. Facilitators should leverage this phase to foster deep discussions about the challenges of evaluation, the importance of explicit evidence, and the impact of clarity on a "pass/fail" decision. This experience directly feeds into Phase 4's reflection on "Evaluator Experience" and "Takeaways & Application," solidifying the practical lessons learned from being on both sides of the procurement table. Facilitators should view Phase 4 not merely as a presentation, but as a critical assessment point for the program itself. The quality and depth of participant reflections can provide invaluable feedback for continuous improvement of the DITAP curriculum. It also serves as the final, reinforcing loop for the program's core message: bridging the gap between government and industry in digital services procurement. The "Evaluator Experience" and "Takeaways & Application" sections are particularly important for measuring the transfer of learning to real-world roles.

### Team Deliverables: 
The primary deliverable for this phase is the Final Team Presentation and an LDA Retrospective.

### Assessment and Evaluation Framework

The overall assessment of the Live Digital Assignment is broken down into four weighted components:
* Case Study Package: 35%  
* Peer Evaluation: 25%  
* Presentation: 25%  
* Reflections/Assessment: 15%

Facilitators are responsible for applying this assessment breakdown consistently across all teams, ensuring fairness and objectivity in grading. It is important to reiterate that the "Pass/Fail" evaluation in Phase 2 for the Case Study Package serves as a critical gate; successful completion of this phase is a prerequisite for further assessment and progression through the LDA.

## DITAP LDA Impact Summary

| Area of Growth | How LDA Delivers It |
| :---- | :---- |
| Empathy | Simulates vendor constraints & storytelling |
| Stakeholder Engagement | Research & interviews with real companies |
| Communication | Proposal writing, presentations, peer review |
| Agile & HCD Literacy | Aligns with Agile & user-centered methods |
| Technical Evaluation | Rubric-based peer evaluations |
| Readiness | Mirrors real government procurement cycles |

## Conclusion

The Vendor Evaluation Live Digital Assignment for the DITAP training course represents a significant evolution in experiential learning for government acquisition professionals. By integrating independent acquisition discovery with a rigorous, fixed Case Study requirement aligned with the Case Study Evaluation framework, the program provides an unparalleled simulation of the digital services procurement lifecycle. This comprehensive approach not only builds essential technical skills in Agile procurement and evaluation but also cultivates a profound sense of empathy for vendors, a critical attribute for fostering more collaborative and successful government-industry partnerships. The structured, phase-based progression, coupled with the emphasis on self-directed learning and peer feedback, ensures that DITAP graduates are not merely knowledgeable but are truly ready to lead the transformation of digital service acquisition within their respective agencies. 

