# Vendor FAQs

Q. Will pre- and post-assessments use the same questions for comparative measurement?

>A. The pre- and post-assessments do not use identical questions, but they are designed to be comparable. The Pre-Assessment surfaces baseline understanding and confidence with DITAP tools and frameworks, helping facilitators anticipate learner needs and tailor early instruction. The Post-Assessment captures growth in conceptual fluency, confidence, and applied decision-making, giving participants a clear view of their progress. Baseline assessments are provided, and vendors may amend scenarios or questions, but they must ensure consistent administration across cohorts for reliable comparisons.

---

Q. For pass/fail, are you recommending a threshold above which you've passed or below which you've failed? 
>A. DITAP’s pass/fail approach represents a major mindset shift for participants, vendors, and facilitators. Many colleges, graduate programs, and professional schools are moving away from point-based grading because adult learning research shows that numerical scores don’t necessarily reflect whether someone can competently do the work. This model reduces performance anxiety, fosters collaboration, and shifts the focus from chasing points to building real capability. 
>
>But the mindset shift is very real.
>
>Instead of a single cutoff score like 70% or 80%, this refreshed grading perspective for DITAP uses multiple checkpoints and clear criteria so participants always know where they stand on the road to passing:
>* **Participation:** Measured using a shared rubric (Fully Engaged, Progressing, Needs Attention). Two consecutive “Needs Attention” ratings in the same dimension or three total across any dimension trigger a facilitator check-in and support plan.
>* **Conceptual Fluency:**
>   * Knowledge checks: Ungraded (not part of pass fail criteria), participants get a sense of their learning and vendors use to tailor content and support.
>   * Stakeholder research
>      * Stage 1: Approval
>      * Stage 2: Pass/Fail on reflection depth
>   * Shadowing
>   *   * Stage 1: Approval
>       * Stage 2: Pass/Fail on reflection depth
>   * Live Digital Assignment
>     * Case study package is gated pass/ fail (high stakes)
>   * Post Assessment
>     * 80% correct to pass
>     * Multiple attempts allowed with vendor support
>
> This approach ensures there are no surprises. Participants get feedback early and often, and they know exactly what actions to take to stay on track.
> <br>
> 
> Remember, numerical scores don’t always equate to job readiness. Whether it's medicine or government procurement, what matters most is whether professionals can perform competently in real scenarios. DITAP’s approach gives participants every chance to demonstrate that they can do the work, not just pass a test.

---

Q. Will module-level (formerly release-level) assessments be updated? 
>A. End-of-module assessments are at vendor discretion, but knowledge checks and applied assignments embedded in modules are a required part of assessing conceptual fluency (See: Vendor Playbook \- Assessments \- Vendor Assessment Requirements Summary). Vendors may update or supplement these as long as they reinforce module objectives and align with the core assessment areas.

---

Q. Will there be a consistent point system (e.g., \~1000 total points, 70% passing threshold)? 
>A. The DITAP program uses a pass/fail structure rather than a point system. This approach emphasizes meaningful growth, collaboration, and real world application, helping learners build lasting competency with less performance anxiety.

---

Q. How will the approach to assessments affect vendor tracking of learner performance and outcome equivalency?
>A. Vendors must track participation, conceptual fluency, and capstone performance using the rubrics and formative/summative assessments outlined in the Assessments section of the Vendor Playbook. While implementation details may vary, the use of consistent rubrics (See: Vendor Playbook \- Assessments \- Participation and Conceptual Fluency) and structured capstone deliverables ensures outcome equivalency across cohorts, in accordance with the Vendor Assessment Requirements Summary outlined in the Vendor Playbook. 

---

Q. Do the pre and post assessments count towards the final pass/fail system for the program?
>A. **Pre-Assessment: Informational & Diagnostic**
The pre-assessment is informational only for both learners and vendors. Participants receive a score, but they can continue to the course regardless of their score. The goal is to surface baseline understanding, build learner awareness of what’s ahead, and allow vendors to identify trends that might signal where additional instructional emphasis or support will be helpful.
>
>**Post-Assessment: Part of Pass/Fail Determination**
The post-assessment is a required component of the pass/fail system. It confirms that participants have reached the minimum level of conceptual fluency necessary to demonstrate mastery of DITAP frameworks.
>
>Best practice recommendations:
>* Learners should be allowed more than one attempt to pass (80% or above).
>* After a failed attempt, redirect the learner to the sections or modules where they struggled, or schedule a quick facilitator check-in to review key concepts.
>* After multiple unsuccessful attempts, vendors should consider additional interventions (peer coaching, office hours, targeted practice) to ensure the learner isn’t just passing the test but can apply the concepts in a real-world setting.

---

Q. Will we be able to get to Level 8 in the Learning-Transfer Evaluation Model (LTEM)?
>A. Level 8 is really the gold standard for measuring learning impact — it asks the ultimate question: _Did this training actually improve mission outcomes?_
>
>That’s a very high bar. To get there, we’d have to separate the impact of DITAP from everything else happening inside an agency — leadership changes, policy shifts, budget fluctuations, and other initiatives that may have run at the same time. There’s also a time lag: by the time results show up, learners may have switched roles or priorities may have shifted, making it hard to draw a straight line back to this course.
>
>Think about trying to prove that one leadership course caused a measurable change in procurement cycle time. It’s possible, but it’s complex, and it requires a lot of data we may not have access to.
>
>For single-agency cohorts, Level 8 is more achievable because we can get a clearer picture of that agency’s environment and sometimes access metrics on procurement outcomes or delivery improvements.
>
>We focus on getting as close as possible — ensuring learners demonstrate task competence in realistic settings (Level 6). And the consortium will be focused on assessing the transfer of those skills back on the job in a post survey (Level 7). If we can measure those reliably, we know we’re setting the stage for Level 8 outcomes, even if we can’t always capture organizational results.

---

Q. How will program duration (3 vs. 6 months) affect CLP eligibility (e.g., 60 vs. 80 CLPs)?
>A. Participants in the full 6-month program continue to be eligible for 80 CLPs, the same as in the previous program. For shorter or adapted versions, training providers must work with USDS to determine the appropriate CLP allocation. Clear criteria should be communicated in course materials to ensure learners understand the difference.

---

Q. Should curriculum materials be publicly available if vendor eligibility is limited? 
>A. USDS is making the curriculum open source as it believes sharing this information with as wide an audience as possible is in everyone’s best interests. However, making the curriculum open source does not impact which vendors are authorized to deliver the training. 

---

Q. Could students access assessments or answers through GitHub, compromising instructional integrity? 
>A. DITAP assessments are experiential in nature, focusing on applied learning, case-based assignments, and real-world decision-making rather than rote memorization; the risk of compromising instructional integrity is lower than in traditional assessment models. Knowledge checks and capstone assignments are designed to reinforce practice and application, not to be memorized. Still, vendors should protect assessment materials where possible to preserve fairness and instructional value.

---


Q. Some vendors have experienced difficulties with the SCORM files in Canvas and Moodle, for example, the content looked "broken" when uploaded. They also found places where they are not passing accessibility tests; the tab order did not perform properly when tested. 
>A. We understand how important it is that the SCORM exports function across different LMSs while also meeting accessibility requirements.
>
> Here is what our team recommends as next steps.
>
> * **Engage LiaScript developers**: Since the accessibility issue appears to be rooted in LiaScript itself (rather than Canvas or the SCORM package), the most direct path may be to raise this with the LiaScript developers. You can open a support issue at this link with LiaScript. We have reached out proactively to our LiaScript contact to let them know this request is likely incoming from you. In our experience, they have been extremely responsive.
>
> * **Accessibility review**: We recommend involving a 508/accessibility expert to validate the behavior and confirm whether there are interim workarounds or if this presents a hard stop. 
>
> * **Canvas technical support**: Canvas does advertise compatibility with both SCORM 1.2 and 2004 exports. We recommend checking with their support team to see if there are recommended configurations or settings that could address the navigation and screen reader issue. 
>
> * **Alternative approach**: If SCORM remains problematic, another option would be to bypass the SCORM export altogether and rebuild the content directly in Canvas using the markdown files. This would allow for more direct control over accessibility and alignment with your LMS environment.
> * **Tip**: Using H5P plugins may hinder the functionality in Moodle and other LMSs. 

---

Q. Some Applied Learning assignments (like the Live Digital Assignment) end before the overall program, and others may fall right after certain modules. How should vendors handle these timelines?
>A. Vendors have flexibility to adjust assignment timelines to fit their delivery needs. The key is ensuring that all assignment requirements outlined in the Vendor Playbook are met, and that participants complete each assignment as instructed.

---

Q. Sprint 4 / Cynuria: Only seeing one place mention the “FAR 10 Integration Assignment” (Vendor Materials > Vendor Playbook > Vendor Playbook.md)
>A. Please see the Module 2 Sprint 4 in-class materials folder for the document "2_Case Study: Far 10 Integration".

---

Q. Unclear if the Post-Assessment is completed during Mod5 Sprint 2 or after it concludes.
>A. Complete the Post-Assessment at the very end of the module. 
