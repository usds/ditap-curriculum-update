# Evaluating Vendors for Agile Delivery 

## Evaluating Vendors for Mission Fit 

When it comes to acquiring digital services, the evaluation phase isn‚Äôt just about compliance‚Äîit‚Äôs your opportunity to find the right team. In agile procurements, success depends on selecting vendors who can think creatively, collaborate with government staff, and deliver working solutions that meet real user needs.

In this section, you‚Äôll learn how to design evaluation strategies that move beyond paperwork. You‚Äôll explore techniques such as live demos, collaborative interviews, and phased-down selects that reveal how vendors work‚Äînot just what they claim.

You‚Äôll also see how agencies like the VA have used these approaches to modernize services, and you‚Äôll walk away with practical tools for shaping evaluation criteria that align with agile principles and mission outcomes.

**What‚Äôs Different About Evaluating Agile Proposals?**

- **Focus on outcomes, not just plans.** Agile acquisitions prioritize vendors who can **show**, not just **tell**. Demos, prototypes, or working code are more useful than polished slide decks.
- **Experience matters differently.**
  - **Past Experience** shows what the vendor built and how they worked.
  - **Past Performance** is more about their track record in following contract terms.
- **Interactive evaluations help.** Agile evaluations may include live demos, team interviews, or problem-solving sessions, providing you with real insight into how the vendor thinks and collaborates.

**Multi-Phase Approach (a.k.a. Down-Select)**

To avoid long proposal reviews and reduce protest risk:
1. Start with a light lift (e.g., short white paper, case studies).
2. Narrow the field based on fit.
3. Invite top vendors to demo or interview. Try hosting a design or code challenge\!
4. Choose the best team through real interaction, not just paperwork.

**Choosing an Evaluation Method**

What Makes a Good Evaluation Criterion?
- **Clear and measurable** (e.g., ‚ÄúDelivered 3+ federal Agile products in the last 2 years‚Äù)
- **Aligned to your goals** (e.g., fast delivery, modular build, user-centered design)
- **Fair and defensible** (complies with agency policy and FAR)

**üìÑ Read: [VA.gov Modernization Case Study](https://github.com/usds/ditap-curriculum-update/blob/main/3_Curriculum/3C_DITAP-Adaptation-Curriculum/3C.1_DITAP-Product-Thinking-And-Acquistions-Curriculum/Module%203/Artifact%3A%20VA.gov%20Modernization%20Case%20Study.md)**

Now that you‚Äôve explored different evaluation approaches, revisit the VA.gov Modernization Case Study. Pay close attention to how the VA team used a multi-phase evaluation strategy‚Äîshaped by discovery insights‚Äîto select a vendor team that could collaborate, prototype, and deliver in real-time. As you read, consider: What evaluation methods did they use to go beyond written proposals? How did their evaluation criteria reflect agile and user-centered delivery needs? What lessons might apply to your own agency‚Äôs procurement process?

**1\. Start with the problem, not the template:** Instead of starting with boilerplate evaluation criteria, teach PMs to clarify:
- What do we need to learn about vendors?
- What do we want to see them **do**, not just claim?

**Insight:** If you had two weeks to test-drive a team, what would you want to observe?

**2\. Use a down-select to focus on the best teams:** A down-select (multi-phase evaluation) filters early to avoid wasting vendor or government time.

**Phase Examples:**
- **Phase 1:** Written *case studies* showing real work
- **Phase 2:** Short written plan \+ team interviews (non-evaluated Q\&A)
- **Phase 3:** **Live working session** or demo (the "show, don‚Äôt tell" part)

**Insight:** Early phases focus on screening for relevant experience and mindset; later phases assess delivery behaviors.

**3\. Design an Evaluation that Tests Real-World Fit:** Every evaluation activity should simulate an aspect of **real collaboration**:
- Can they understand a problem quickly?
- Can they communicate with non-tech stakeholders?
- Can they co-create with a product team?

**VA Example:** Vendors had to build a prototype in 4 hours, collaborate with actors playing VA staff, and document their thinking process.

**4\. Use Comparative Evaluation:** Instead of scoring checkboxes, use **comparative analysis**:
- What makes this vendor‚Äôs solution better or worse than another‚Äôs?
- How did they perform *relative to others*?
  
**Insight:** Think about user testing: not ‚ÄúDid they meet 5 criteria,‚Äù but ‚ÄúWhich team best solves our problem in practice?‚Äù

**5\. Write Clear Instructions to Offerors:** Vague questions lead to vague answers. Provide:
- **Clear deliverables** (e.g., "Submit 3 case studies with A‚ÄìF info")
- **Tangible formats** (e.g., "Prototype hosted on GitHub")
- **Focus areas** (e.g,. "Show how you would manage content ops and user research")

**Tip:** Give page limits, scenario-based prompts, and expected artifacts.

**6\. Coach Your Evaluation Team:** PMs should help orient evaluators around outcomes:
- What are we looking for during a demo?
- How will we document strengths/weaknesses?
- Who‚Äôs responsible for each evaluation category?

**VA Lesson:** Align the team early; if one evaluator expects flashy design and another looks for clean code, the decision will be misaligned.

**Participation is key**

Product Managers, technologists, and CORs play an essential role in designing effective evaluation strategies. Their insight ensures that acquisition activities support real product delivery‚Äînot just compliance. To make the most of the evaluation process:
- Be involved in shaping evaluation criteria that reflect what success looks like in real-world delivery.
- Ensure each phase (e.g., case studies, technical demos) is tied to specific product needs.
- Help define meaningful artifacts that reflect working software, not just polished writing.
- Engage the Contracting Officer (CO) at every step. Early and ongoing collaboration ensures that the acquisition strategy, evaluation criteria, and communication with vendors remain legally sound and mission-aligned.

Activity 3</br>
**Insert Activity** ‚Äì Using the same teams in Activity 1, develop an evaluation plan.

## Claass Activity: [Designing an Agile Evaluation Strategy](https://github.com/usds/ditap-curriculum-update/blob/main/3_Curriculum/3C_DITAP-Adaptation-Curriculum/3C.1_DITAP-Product-Thinking-And-Acquistions-Curriculum/Module%203/Class%20Activity%3A%20Designing%20an%20Agile%20Evaluation%20Strategy.md) 

In this final activity, participants will design a high-level evaluation strategy for the ESA Registry procurement. Building on their earlier work (contracting approach and cost estimate), they‚Äôll outline a phased approach to vendor evaluation, aligned with agile principles and agency readiness. This exercise emphasizes collaboration, iteration, and selecting vendors based on their ability to deliver real user value.

