# 

# 

# 

# DITAP Curriculum Refresh 

## *What's Changed, What's Stayed, and What Vendors Need to Know*

## 

## 

## 

## 

## 

## 

# 

# Executive Summary

The DITAP curriculum refresh represents a comprehensive modernization of the program, driven by evidence-based research and stakeholder feedback. This guide provides vendors with essential information about curriculum changes, implementation requirements, and new resources to ensure successful program delivery.

## 

## What Changed and Why

### Research Foundation 

Three key reports from our curriculum audit and user research identified what needed to change:

* **Gap Analysis Report**: Identified specific knowledge and skill gaps in current delivery  
* **Audit Findings Report**: Highlighted structural and content inconsistencies  
* **User Research**: Captured feedback from current participants, alumni, and stakeholders

### Key Findings 

Participants needed more hands-on learning, current technology content, elimination of outdated material, and reduced cognitive load for working professionals.

## 

## Major Changes Implemented:

Based on this research, we made five key improvements:

* **Structure**: Moved from 6 releases to 5 focused modules with sprint-based organization  
* **Learning Approach**: Enhanced experiential learning with threaded case studies throughout modules  
* **Technology**: Added dedicated Technology Bootcamp sprint covering AI, open source strategies, and accessibility  
* **Content**: Updated to plain language with full 508 compliance  
* **Support Materials**: Created materials including Facilitator, Participant and Facilitator Onboarding Kit to equip DITAP vendors with the resources, skills, and tools needed for effective program delivery

DITAP now delivers more practical, relevant training that better serves working professionals while ensuring consistent quality across all DITAP vendors.

This document details each change, explains the research behind our decisions, and shows how improvements benefit DITAP participants and delivery partners.

# Structural Changes

### 

## Terminology and Organization Updates

### From "Releases" to "Modules" 

The program now uses "Modules" instead of "Releases" to better align with standard educational terminology and improve clarity for both facilitators and learners.

### New Module Structure: Describe → Discover → Design → Build → Grow 

The five modules create a logical progression tied directly to revised performance outcomes:

| Module | Focus | Key Outcome |
| ----- | ----- | ----- |
| **Describe** | Digital services fundamentals | Define and contextualize digital acquisition landscape |
| **Discover** | Stakeholder analysis and market research | Assess needs and identify appropriate solutions |
| **Design** | Acquisition strategy development | Create comprehensive acquisition packages |
| **Build** | Contract execution and management | Manage digital service delivery effectively |
| **Grow** | Change leadership and influence | Champion digital transformation across agencies |

### Sprint-Based Content Organization 

Within each module, content is organized into focused "sprints" that:

* Align with specific learning outcomes

* Provide flexibility for delivery scheduling

* Support modular instruction while maintaining consistency

* Enable easier progress tracking and assessment

# 

### Competency Integration

While the program has always been built around six core competencies, the sixth focused on applying skills was previously presented as a standalone module. In the refresh, we have taken a more integrated approach, embedding opportunities to practice and apply skills across all modules. This ensures learners reinforce the competency in context, at multiple points in the program, rather than in a single, isolated module.

# 

# Content Changes

### 

## What Has Largely Remained the Same

Several core elements have been preserved based on positive stakeholder feedback and guidance from a pool of procurement SMEs:

### Foundational Concepts:

* Digital services definitions and problem-solving applications

* Stakeholder analysis methodologies

* FAR flexibility identification and application

* Market research techniques and strategies

* Change leadership principles

### Delivery Approach:

* Small team collaborative structure

* Focus on practical application over theoretical knowledge

* Integration of real-world scenarios and challenges

### 

## Major Content Updates

#### 

### Technology Modernization

**Previous Approach:** Emphasis on cloud as the primary defining technology 

**New Approach:** Update what is considered to be emerging technology

**Specific Enhancements:**

* **Artificial Intelligence**: Practical applications in federal acquisition, ethical considerations, and implementation strategies

* **Open Source Strategies**: Evaluation criteria, security considerations, and integration approaches

* **Accessibility**: 508 compliance integration, inclusive design principles, and assistive technology considerations

**Rationale:** Gap Analysis Report indicated that cloud-centric content had become outdated, while alumni feedback consistently requested coverage of emerging technologies critical to modern digital services.

#### 

### Enhanced Experiential Learning

**Previous Approach:** Traditional lecture-based content with limited hands-on activities 

**New Approach:** Integrated experiential learning throughout all modules

**Specific Enhancements:**

* **Interactive Activities**: Structured exercises in each sprint that reinforce key concepts

* **Participant Discussions**: Facilitated conversations that leverage diverse agency experiences

* **Self-Reflection Components**: Regular checkpoints for learners to assess understanding and application

* **Skills-Based Assessments**: Practical evaluations that demonstrate competency rather than recall

#### 

### Threaded  Case Study Approach

**Previous Approach:** Multiple scenarios across modules 

**New Approach:** Single, comprehensive case study that threads through the entire program

**Benefits for Vendors:**

* Provides consistent reference point for all instructional activities

* Reduces preparation time by focusing on one detailed scenario

* Enables progressive skill building as learners apply each module's concepts to the same situation

* Creates more coherent learning experience with cumulative impact

## 

# Learning Objective Analysis by Module

### 

## Module 1 (Describe) \- Digital Services Fundamentals

**Approach:** Consolidated and clarified foundational concepts

**What Changed:**

* Combined related objectives to reduce redundancy

* Enhanced focus on Digital Service Playbook and TechFAR Handbook integration

* Added emphasis on major technology areas and modern delivery methods

**What Remained:**

* Core digital services definitions

* Stakeholder identification in digital ecosystem

* Professional role articulation

**Implementation Impact:** Facilitators should focus on integrated concept delivery rather than discrete topic coverage.

### 

## Module 2 (Discover) \- Market Research and Stakeholder Analysis

**Approach:** Maintained core analytical skills with enhanced practical application

**What Changed:**

* Streamlined from 7 to 4 learning objectives

* Removed communication protocols content (integrated elsewhere)

* Enhanced focus on effective questioning techniques

**What Remained:**

* Stakeholder analysis methodologies

* Agency readiness assessment approaches

* Market research and intelligence strategies

**Implementation Impact:** More time available for in-depth practice of analytical skills.

### 

## Module 3 (Design) \- Acquisition Strategy Development

**Approach:** Focused on core strategy development with reduced peripheral content

**What Changed:**

* Eliminated change lifecycle communication strategies

* Removed detailed coverage of challenge processes

* Simplified evaluation criteria focus

**What Remained:**

* FAR flexibility applications

* Evaluation methods and criteria development

* Acquisition package development fundamentals

**Implementation Impact:** Concentrate delivery on essential strategy development skills.

## Module 4 (Build) \- Contract Management and Execution

**Approach:** Significantly enhanced focus on agile delivery and product ownership

**What Changed:**

* Added comprehensive product ownership framework

* Enhanced COR/PO interaction guidance

* Integrated metrics creation and utilization

* Added exit strategy and course correction planning

**What Remained:**

* Contract award transition processes

* Technical evaluation team selection

**Implementation Impact:** Requires additional facilitator preparation on agile methodologies and federal product ownership.

### 

## Module 5 (Grow) \- Change Leadership and Influence

**Approach:** Streamlined influence strategies with maintained core change leadership content

**What Changed:**

* Consolidated influence-related objectives

* Removed some tactical conversation strategies

* Maintained strategic change planning focus

**What Remained:**

* Spheres of influence identification

* Change agent characteristics and strategies

* Action planning for organizational transformation

**Implementation Impact:** Focus on high-level strategic planning rather than detailed tactical approaches.

# User Experience and Accessibility Improvements

### 

## Plain Language Compliance

In accordance with the Federal Plain Writing Act of 2010, all content has been reviewed and revised using:

* Hemingway app analysis for grade-level comprehension

* Plain language principles throughout

* Improved clarity and accessibility for all learners

## 

## Visual and Structural Enhancements

**DITAP Content Style Guide Implementation:**

* Consistent typography and content layouts across all modules

* Proper heading structure for screen readers and assistive technology

* Streamlined visual elements for better navigation

* Unified voice and tone throughout the program

### 

### **Accessibility Features**

* Enhanced support for learners using assistive technology

* Improved asynchronous review capabilities

* Consistent content structure for easier navigation

* Clear visual hierarchy and formatting

## 

# Assessment Framework Transformation

### 

## Philosophy Shift: From Traditional Grading to Adult Learning-Centered Assessment

The DITAP refresh introduces a comprehensive assessment overhaul that moves beyond traditional academic models toward an approach rooted in adult learning and experiential learning principles to support meaningful professional development. This represents one of the most significant changes vendors must prepare for.

**Previous Assessment Approach:**

* Traditional point-based grading systems

* Limited feedback mechanisms

* Focus on knowledge retention over application

* Inconsistent evaluation criteria across vendors

**New Assessment Philosophy:**

* Pass/fail structure designed to reduce anxiety and emphasize growth, collaboration, and real-world application

* Continuous formative feedback with scaffolded support and early intervention

* Focus on building habits and mindsets for ongoing professional development

* Assessment design grounded in the Learning-Transfer Evaluation Model (LTEM) to measure decision-making, performance in realistic settings, and readiness for on-the-job transfer


### 

## Three-Pillar Assessment Framework

All participants must demonstrate competency across three important areas:

### 1\. Participation (Recommended 40% Weight)

#### Assessment Dimensions:

* **Cohort Contribution**: Active engagement, building on others' ideas, preparation level

* **Connections**: Linking course content to real work contexts and sharing relevant examples

* **Emergent Thinking**: Introducing new perspectives and demonstrating intellectual curiosity

#### Assessment Methods:

* Self-assessment (once per module)

* Peer feedback (twice per course)

* Facilitator observation (twice per course)

#### Vendor Implementation Requirements:

* Use standardized participation rubric with three levels: Fully Engaged, Progressing, Needs Attention

* Implement mandatory check-in protocols when learners receive two consecutive "Needs Attention" ratings OR three total across any dimension  
* Document participation patterns to inform ongoing support and intervention strategies


### 2\. Conceptual Fluency (Recommended 30% Weight)

#### Demonstrates learner ability to:

* Understand and apply key DITAP frameworks (Digital Services Playbook, TechFAR, Human-Centered Design principles)

* Connect theoretical concepts to practical acquisition scenarios in realistic contexts

* Communicate complex ideas clearly in professional contexts

#### Conceptual Fluency Assessment Components:

* Pre- and post-assessments 

  * Pre- and post-course assessments bookend the DITAP learning experience, helping participants and facilitators measure growth, identify focus areas, and align learning activities to real-world application. While neither is graded, they provide valuable benchmarks that guide ongoing support and reflection.  
  * Pre-Assessment: Surfaces baseline understanding and confidence with DITAP tools and frameworks. Helps facilitators anticipate support needs and tailor early instruction.  
  * Post-Assessment: Captures growth in conceptual fluency, confidence, and applied decision-making. Comparing results offers participants a clear view of their progress.  
  * Baseline assessments are provided. Vendors may amend scenarios or questions but should ensure consistent administration for comparable results across cohorts.  
* Knowledge checks embedded in self-paced content

  * Formative, Not Summative: Low-stakes and non-graded; their purpose is reflection and reinforcement, not compliance

  * Integrated for Reinforcement:  Aligned to module objectives and placed where content is most critical to help learners connect theory to practice in real time.

  * Flexible Implementation: May be referenced in live facilitation, supplemented with additional checks, or adapted for agency-specific scenarios.

* Applied Learning Assignments and In-Class Discussions including:

  * Stakeholder Research Assignment  
  * Shadowing Assignment  
  * In-Class Discussions of the Threaded Case Scenario

### Recommended Cadence:

* **Bi-weekly:** Provide feedback on assignments and self-paced work.  
* **End of Module:** Assign a formal rating and document progress.  
* **Ongoing:** Act on repeated "Needs Attention" patterns immediately.

#### 

### 3\. Live Digital Assignment (Recommended 30% Weight)

#### Four-Phase Comprehensive Experience:

1. **Discovery Sprint**: Problem framing through stakeholder interviews

2. **Case Study Development**: Creation of four artifacts aligned to SPRUCE Technical Factor

3. **Peer Evaluation**: Blind review and feedback using standardized rubrics

4. **Final Presentations**: Team reflections and learning synthesis

**The following is a suggested weighting for assessments within the LDA. Vendors may adjust these percentages at their discretion.**

* Case Study Package: 35%

* Peer Evaluation: 25%

* Team Presentation: 25%

* Team Reflection: 15%

## 

## Suggested Assessment Requirements for Vendors

In summary, each vendor must implement an assessment plan that supports the pass/fail nature of the course and includes:

* **Meaningful participation measurement** using engagement rubrics and opportunities for ongoing feedback.  
* **Consistent informal and formal measurement of conceptual fluency** across multiple course elements, including:  
  * Administration of pre-course and post-course assessments  
  * Knowledge checks embedded in the self-paced modules  
  * Applied learning assignments such as shadowing and stakeholder research and in-class discussions such as those related to the threaded case scenario  
* **A cumulative, culminating applied learning assignment** that simulates real-world digital acquisition challenges.

Vendors may supplement with additional assessments based on their delivery format, learner preferences, or agency-specific use cases.

### 

## Quality Assurance Standards

**Assessment Timing:** All assessments must be completed within specified time windows to enable cohort-level analysis

**Transparency Requirements**: Provide examples of high-quality work and clear success criteria for all assignments

**Remediation Protocols**: Structured support plans for learners requiring additional assistance and 

**Support Tools:** Assessment rubric training and calibration exercises and troubleshooting guides for common assessment challenges

## 

# New Resources and Support Materials

## 

## The DITAP Digital Workbook

**Purpose:** Comprehensive learning support that integrates seamlessly with the new assessment framework

**Features:**

* Integrated case studies and exercises aligned to assessment criteria

* Self-reflection prompts supporting the participation evaluation framework

* Practice scenarios for conceptual fluency development

* LMS integration capabilities for progress tracking

* Consistent experience across vendor implementations

**Assessment Integration:**

* Built-in self-assessment tools aligned to participation rubric

* Practice exercises that feed into summative assessments

* Progress tracking capabilities for facilitator monitoring

### 

## Train-the-Trainer Materials

### 

### Components:

* Comprehensive facilitator walkthrough that demonstrates how the guides are set up

* Best practice guidance for experiential learning delivery and evaluation

* Sample delivery schedule template

* Best practices for delivery facilitation

### Assessment-Specific Training:

* Pass/fail mindset and communication strategies

* Effective feedback delivery techniques

* Early intervention protocols for struggling learners

* Documentation requirements for program completion decisions

### 

### 

## Live Digital Assignment (LDA) Facilitator Guide

### 

### Comprehensive Resource Including:

* Phase-by-phase implementation instructions

* Assessment templates and rubrics aligned to SPRUCE Technical Factor

* Peer evaluation frameworks and blind review processes

* Presentation evaluation criteria and reflection prompts

* Sample timelines and facilitation strategies

### 

## Technical Documentation

### LiaScript Implementation Guide:

* GitHub-based development workflow with version control

* SCORM compliance generation for LMS integration

* Assessment tracking and reporting capabilities

* Progress monitoring and analytics tools

# Expected Outcomes and Benefits

## For Learners

* More practical, applicable skills for immediate workplace implementation

* Reduced performance anxiety through growth-focused assessment approach

* Enhanced understanding of modern digital acquisition challenges

* Continuous feedback and support throughout the learning process

* Improved confidence in managing complex acquisition scenarios through applied practice

* Better preparation for change leadership roles with demonstrated competency

## For Agencies

* More capable acquisition professionals with current technology knowledge and demonstrated practical skills

* Clear evidence of learner competency through comprehensive assessment framework

* Improved digital service procurement outcomes based on applied learning experiences

* Enhanced organizational change capacity through skilled change agents

* Better alignment with government-wide digital transformation initiatives

* Consistent professional development outcomes across all vendor implementations

## For Vendors

* Clearer instructional framework with reduced preparation requirements

* Enhanced learner satisfaction and engagement

* More consistent delivery experience across different facilitators

* Improved program effectiveness measures

*This guide represents the foundational information for curriculum refresh implementation. Additional detailed materials, including specific exercise instructions and assessment rubrics, are available through the USDS GitHub repository.*